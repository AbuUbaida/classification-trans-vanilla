The main goal was to explore the transformer from ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) from scratch and implement it for text classification. For this task, the [AG news classification dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset) was utilized, which is constructed by choosing 4 largest classes from the [original corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html). The transformer network implementation was based on the [annotated version](http://nlp.seas.harvard.edu/2018/04/03/attention.html) from Harvard NLP, but some modifications were made to work for the text classification problem. Only the encoder part was used, residual connection and layer normalization were skipped, and the masking was not considered either. For extracting the features, multihead attention and positionwise feedforward network were incorporated, and finally, a linear layer was added to get the logits.
